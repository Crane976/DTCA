# models/train_mlp_hunter.py (FINAL FIXED VERSION With Data Cleaning)
import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from sklearn.metrics import classification_report, f1_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm

# Path Setup
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

from config import DEFENDER_SET, set_seed
from models.mlp_architecture import MLP_Classifier, FocalLoss

# Configuration
TRAIN_SET_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
TEST_SET_PATH = os.path.join(project_root, 'data', 'splits', 'holdout_test_set.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
MLP_HUNTER_MODEL_PATH = os.path.join(project_root, 'models', 'mlp_hunter.pt')

FEATURE_DIM = len(DEFENDER_SET)
EPOCHS = 100
BATCH_SIZE = 256
VALIDATION_SPLIT = 0.2
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
RANDOM_SEED = 2025
BEST_PARAMS = {'learning_rate': 0.0005}


def clean_data(df, feature_cols):
    """ç»Ÿä¸€çš„æ•°æ®æ¸…æ´—å‡½æ•°: æ›¿æ¢Infå¹¶ä¸¢å¼ƒNaN"""
    # 1. æ›¿æ¢ Inf
    df.replace([np.inf, -np.inf], np.nan, inplace=True)
    # 2. ä¸¢å¼ƒåŒ…å«NaNçš„è¡Œ (ä»…æ£€æŸ¥ç‰¹å¾åˆ—)
    df.dropna(subset=feature_cols, inplace=True)
    return df


def main():
    set_seed(RANDOM_SEED)
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ ResNet-MLP Hunter (ä¿®å¤ç‰ˆ: å«æ•°æ®æ¸…æ´—)...")
    print("=" * 60)

    # --- 1. åŠ è½½æ•°æ® ---
    df_train_full = pd.read_csv(TRAIN_SET_PATH)
    df_test = pd.read_csv(TEST_SET_PATH)
    scaler = joblib.load(SCALER_PATH)
    feature_names = scaler.feature_names_in_

    # --- âœ… å…³é”®ä¿®å¤: æ•°æ®æ¸…æ´— ---
    print("æ­£åœ¨æ¸…æ´—æ•°æ® (å»é™¤ Inf/NaN)...")
    len_train_before = len(df_train_full)
    df_train_full = clean_data(df_train_full, feature_names)
    print(f"   -> è®­ç»ƒé›†æ¸…æ´—æ‰ {len_train_before - len(df_train_full)} æ¡è„æ•°æ®")

    len_test_before = len(df_test)
    df_test = clean_data(df_test, feature_names)
    print(f"   -> æµ‹è¯•é›†æ¸…æ´—æ‰ {len_test_before - len(df_test)} æ¡è„æ•°æ®")

    # --- 2. è½¬æ¢ä¸åˆ’åˆ† ---
    X_test_scaled = scaler.transform(df_test[feature_names].values)
    y_test = df_test['label'].values

    X_train_full_scaled = scaler.transform(df_train_full[feature_names].values)
    y_train_full = df_train_full['label'].values

    X_train, X_val, y_train, y_val = train_test_split(
        X_train_full_scaled, y_train_full, test_size=VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=y_train_full
    )

    # --- 3. åŠ æƒé‡‡æ ·å™¨ ---
    class_sample_count = np.array([len(np.where(y_train == t)[0]) for t in np.unique(y_train)])
    weight = 1. / class_sample_count
    samples_weight = np.array([weight[t] for t in y_train.astype(int)])
    samples_weight = torch.from_numpy(samples_weight)
    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))

    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                  torch.tensor(y_train, dtype=torch.float32).unsqueeze(1))
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False)

    val_tensor_x = torch.tensor(X_val, dtype=torch.float32).to(device)
    val_tensor_y = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)

    # --- 4. è®­ç»ƒ ---
    benign_ratio = (y_train_full == 0).sum() / len(y_train_full)
    model = MLP_Classifier(feature_dim=FEATURE_DIM).to(device)
    criterion = FocalLoss(alpha=0.5, gamma=2.0)  # alphaè°ƒä¸º0.5å› ä¸ºç”¨äº†Balanced Sampler
    optimizer = torch.optim.AdamW(model.parameters(), lr=BEST_PARAMS['learning_rate'])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

    print("\n[æ­¥éª¤1] æ­£åœ¨è®­ç»ƒæ¨¡å‹...")
    best_val_loss = float('inf')

    for epoch in range(EPOCHS):
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS}", leave=False)
        for x_batch, y_batch in pbar:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            loss = criterion(model(x_batch), y_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})

        model.eval()
        with torch.no_grad():
            val_loss = criterion(model(val_tensor_x), val_tensor_y).item()
        scheduler.step(val_loss)

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), MLP_HUNTER_MODEL_PATH)

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")

    # --- 5. é˜ˆå€¼ä¸è¯„ä¼° ---
    print("\n[æ­¥éª¤2] å¯»æ‰¾æœ€ä½³å†³ç­–é˜ˆå€¼...")
    final_model = MLP_Classifier(feature_dim=FEATURE_DIM).to(device)
    final_model.load_state_dict(torch.load(MLP_HUNTER_MODEL_PATH, map_location=device))
    final_model.eval()

    with torch.no_grad():
        val_probs = final_model.predict(val_tensor_x).cpu().numpy()

    best_threshold = 0.5
    best_f1 = 0
    for threshold in np.arange(0.01, 1.0, 0.01):
        y_val_pred = (val_probs > threshold).astype(int)
        current_f1 = f1_score(y_val, y_val_pred, pos_label=1)
        if current_f1 > best_f1:
            best_f1 = current_f1
            best_threshold = threshold

    print(f"âœ… æœ€ä½³é˜ˆå€¼: {best_threshold:.2f} (F1: {best_f1:.4f})")

    with torch.no_grad():
        test_tensor_x = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)
        test_probs = final_model.predict(test_tensor_x).cpu().numpy()
        y_pred = (test_probs > best_threshold).astype(int)

    print(classification_report(y_test, y_pred, target_names=['Benign (0)', 'Bot (1)'], digits=4))


if __name__ == "__main__":
    main()