# models/STEP2_train_bot_lstm.py
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
# âœ… æ ¸å¿ƒä¿®æ­£: ä»Ž torch.utils.data ä¸­åŒæ—¶å¯¼å…¥ Dataset, DataLoader, TensorDataset
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import os
import joblib
from config import UNIFIED_FEATURE_SET, TARGET_FIELDS_FOR_LSTM

# å¯¼å…¥æˆ‘ä»¬åœ¨STEP1ä¸­å®šä¹‰çš„CAEæ¨¡åž‹ç±»
# ä¸ºäº†æ–¹ä¾¿ï¼Œç›´æŽ¥åœ¨è¿™é‡Œé‡æ–°å®šä¹‰
class ConditionalAE(nn.Module):
    def __init__(self, input_dim, condition_dim, encoding_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim + condition_dim, 32), nn.ReLU(),
            nn.Linear(32, 16), nn.ReLU(),
            nn.Linear(16, encoding_dim)
        )
        self.decoder = nn.Sequential(
            nn.Linear(encoding_dim + condition_dim, 16), nn.ReLU(),
            nn.Linear(16, 32), nn.ReLU(),
            nn.Linear(32, input_dim)
        )

    def forward(self, x, c):
        x_cond = torch.cat([x, c], dim=1);
        encoded = self.encoder(x_cond)
        encoded_cond = torch.cat([encoded, c], dim=1);
        decoded = self.decoder(encoded_cond)
        return decoded, encoded


# å¯¼å…¥æˆ‘ä»¬åœ¨ä¹‹å‰å®šä¹‰çš„LSTMæ¨¡åž‹ç±»
class PredictiveLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2, dropout=0.2)
        self.fc = nn.Sequential(nn.Linear(hidden_dim, 32), nn.ReLU(), nn.Dropout(0.2), nn.Linear(32, output_dim))

    def forward(self, x):
        lstm_out, _ = self.lstm(x);
        last_time_step_out = lstm_out[:, -1, :];
        return self.fc(last_time_step_out)


# --- 1. é…ç½®åŒº ---
DATA_DIR_PREPROCESSED = r'D:\DTCA\data\preprocessed'
DATA_DIR_FILTERED = r'D:\DTCA\data\filtered'
MODELS_DIR = r'D:\DTCA\models'

# --- è¾“å…¥è·¯å¾„ ---
# STEP1è®­ç»ƒå¥½çš„é£Žæ ¼è¿ç§»å¼•æ“Ž
cae_model_path = os.path.join(MODELS_DIR, 'style_transfer_cae.pt')
# LSTMçš„è®­ç»ƒæ•°æ®æº (å·²å¤„ç†çš„Botæµé‡)
bot_processed_path = os.path.join(DATA_DIR_PREPROCESSED, 'bot_traffic_processed.csv')
# LSTMçš„ç›®æ ‡å€¼æ¥æº (åŽŸå§‹çš„Botæµé‡)
raw_bot_csv = os.path.join(DATA_DIR_FILTERED, 'bot_traffic_target.csv')

# --- è¾“å‡ºè·¯å¾„ ---
lstm_model_path = os.path.join(MODELS_DIR, 'bot_pattern_lstm.pt')
target_scaler_path = os.path.join(MODELS_DIR, 'target_scaler_bot.pkl')  # Scaler for Y values

# --- æ¨¡åž‹å‚æ•° ---
input_dim_cae = len(UNIFIED_FEATURE_SET)
encoding_dim = 5
condition_dim = 2
input_dim_lstm = encoding_dim  # LSTMçš„è¾“å…¥æ˜¯CAEçš„ç¼–ç ç»´åº¦

window_size = 3
batch_size = 32
epochs = 100
learning_rate = 0.001
hidden_dim = 64
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
target_fields = TARGET_FIELDS_FOR_LSTM


# --- æ•°æ®é›†å®šä¹‰ ---
class SequenceDataset(Dataset):
    def __init__(self, x, y): self.x = torch.tensor(x, dtype=torch.float32); self.y = torch.tensor(y,
                                                                                                   dtype=torch.float32)

    def __len__(self): return len(self.x)

    def __getitem__(self, idx): return self.x[idx], self.y[idx]


# --- ä¸»è®­ç»ƒå‡½æ•° ---
def main():
    print("=============================================");
    print("ðŸš€ STEP 2: å¼€å§‹è®­ç»ƒ'åŠ¨æ€æ¨¡å¼æ³¨å…¥'LSTMå¼•æ“Ž...");
    print("=============================================");
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # 1. åŠ è½½STEP1çš„CAEæ¨¡åž‹
    print("æ­£åœ¨åŠ è½½'é£Žæ ¼è¿ç§»'CAEå¼•æ“Ž...")
    cae_model = ConditionalAE(input_dim_cae, condition_dim, encoding_dim).to(device)
    cae_model.load_state_dict(torch.load(cae_model_path))
    cae_model.eval()  # æˆ‘ä»¬åªç”¨å®ƒçš„encoderï¼Œæ‰€ä»¥è®¾ä¸ºè¯„ä¼°æ¨¡å¼

    # 2. å‡†å¤‡LSTMçš„è¾“å…¥ (X) å’Œç›®æ ‡ (Y)
    print("æ­£åœ¨å‡†å¤‡LSTMçš„è®­ç»ƒæ•°æ®...")
    df_bot_processed = pd.read_csv(bot_processed_path)
    X_bot_for_encoding = torch.tensor(df_bot_processed.values, dtype=torch.float32).to(device)

    # åˆ›å»ºBotçš„æ¡ä»¶æ ‡ç­¾ [0, 1]
    C_bot_for_encoding = torch.zeros(len(X_bot_for_encoding), condition_dim).to(device)
    C_bot_for_encoding[:, 1] = 1

    # ä½¿ç”¨CAEç¼–ç å™¨æå–Botçš„æ½œåœ¨è¡¨ç¤ºï¼Œä½œä¸ºLSTMçš„è¾“å…¥ç‰¹å¾
    with torch.no_grad():
        _, lstm_input_features = cae_model(X_bot_for_encoding, C_bot_for_encoding)
    lstm_input_features = lstm_input_features.cpu().numpy()

    # å‡†å¤‡LSTMçš„ç›®æ ‡å€¼Y
    df_raw = pd.read_csv(raw_bot_csv);
    df_raw.columns = df_raw.columns.str.strip()
    target_values = df_raw[target_fields].values
    target_scaler = MinMaxScaler();
    target_values_scaled = target_scaler.fit_transform(target_values)
    joblib.dump(target_scaler, target_scaler_path);
    print(f"âœ… ç›®æ ‡å€¼(Y)çš„Bot-Scalerå·²ä¿å­˜åˆ°: {target_scaler_path}")

    # 3. æž„å»ºæ»‘åŠ¨çª—å£åºåˆ—
    print("æ­£åœ¨æž„å»ºæ»‘åŠ¨çª—å£åºåˆ—...")
    X_seq, Y_seq = [], []
    for i in range(len(lstm_input_features) - window_size):
        X_seq.append(lstm_input_features[i:i + window_size]);
        Y_seq.append(target_values_scaled[i + window_size - 1])
    X_seq, Y_seq = np.array(X_seq), np.array(Y_seq)
    X_train, X_val, Y_train, Y_val = train_test_split(X_seq, Y_seq, test_size=0.2, random_state=42)

    # 4. è®­ç»ƒLSTM
    train_loader = DataLoader(SequenceDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(SequenceDataset(X_val, Y_val), batch_size=batch_size)

    output_dim_lstm = len(target_fields)
    lstm_model = PredictiveLSTM(input_dim_lstm, hidden_dim, output_dim_lstm).to(device)
    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=learning_rate);
    criterion = nn.MSELoss()

    print("å¼€å§‹è®­ç»ƒLSTMæ¨¡åž‹...")
    best_val_loss = float('inf')
    for epoch in range(epochs):
        lstm_model.train();
        total_train_loss = 0
        for x_batch, y_batch in train_loader:
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            pred = lstm_model(x_batch);
            loss = criterion(pred, y_batch)
            optimizer.zero_grad();
            loss.backward();
            optimizer.step();
            total_train_loss += loss.item()
        avg_train_loss = total_train_loss / len(train_loader)

        lstm_model.eval();
        total_val_loss = 0
        with torch.no_grad():
            for x_val_batch, y_val_batch in val_loader:
                x_val_batch, y_val_batch = x_val_batch.to(device), y_val_batch.to(device)
                val_pred = lstm_model(x_val_batch);
                val_loss = criterion(val_pred, y_val_batch)
                total_val_loss += val_loss.item()
        avg_val_loss = total_val_loss / len(val_loader)

        if (epoch + 1) % 10 == 0: print(f"  -> Epoch {epoch + 1:3d}/{epochs}, Val Loss: {avg_val_loss:.6f}")
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss;
            torch.save(lstm_model.state_dict(), lstm_model_path)

    print("\n--- è®­ç»ƒå®Œæˆ ---");
    print(f"è¡¨çŽ°æœ€å¥½çš„'åŠ¨æ€æ¨¡å¼æ³¨å…¥'LSTMå¼•æ“Žå·²ä¿å­˜åœ¨: {lstm_model_path}");
    print(f"(Final Best Val Loss: {best_val_loss:.6f})")


if __name__ == "__main__":
    main()