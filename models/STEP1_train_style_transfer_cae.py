# models/STEP1_train_style_transfer_cae.py (FINAL 3-TIER BALANCED & CLEANED VERSION)
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler
from sklearn.model_selection import train_test_split
import os
import sys
import joblib

# ==========================================================
# --- è·¯å¾„ä¿®æ­£ä¸Žæ¨¡å—å¯¼å…¥ ---
# ==========================================================
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.append(project_root)

from models.style_transfer_cae import ConditionalAutoencoder
from config import ATTACKER_KNOWLEDGE_SET, set_seed

# ==========================================================
# --- 1. é…ç½®åŒº ---
# ==========================================================
TRAIN_SET_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
MODELS_DIR = os.path.join(project_root, 'models')
CAE_MODEL_PATH = os.path.join(MODELS_DIR, 'style_transfer_cae.pt')

FEATURE_DIM = len(ATTACKER_KNOWLEDGE_SET)
LATENT_DIM = 5
NUM_CLASSES = 2
EPOCHS = 100
BATCH_SIZE = 128
LEARNING_RATE = 0.001
VALIDATION_SPLIT = 0.2
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ==========================================================
# --- 2. ä¸»è®­ç»ƒå‡½æ•° ---
# ==========================================================
def main():
    set_seed(2025)
    print("==========================================================")
    print("ðŸš€ STEP 1 (Final): è®­ç»ƒä¸Šä¸‹æ–‡æå–CAEå¼•æ“Ž (å‡è¡¡é‡‡æ ·+æ•°æ®æ¸…æ´—ç‰ˆ)...")
    print(f"   >>> æ”»å‡»è€…è®¤çŸ¥è¾¹ç•Œ (è¾“å…¥ç»´åº¦): {FEATURE_DIM} ç»´ <<<")
    print("==========================================================")

    # --- 1. åŠ è½½æ•°æ®å’ŒScaler ---
    print("æ­£åœ¨åŠ è½½è®­ç»ƒé›†å’Œå…¨å±€Scaler...")
    try:
        df_train_full = pd.read_csv(TRAIN_SET_PATH)
        scaler = joblib.load(SCALER_PATH)
    except FileNotFoundError as e:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ ¸å¿ƒæ–‡ä»¶ - {e}");
        return

    # ==========================================================
    # ðŸ§¼ [å…³é”®ä¿®å¤] æ•°æ®å‡€åŒ–æ¨¡å—
    # ==========================================================
    print("æ­£åœ¨è¿›è¡Œæ•°æ®å‡€åŒ– (åŽ»é™¤ Inf/NaN)...")
    original_len = len(df_train_full)

    # 1. æ›¿æ¢ inf ä¸º nan
    df_train_full.replace([np.inf, -np.inf], np.nan, inplace=True)

    # 2. èŽ·å–Scaleræ‰€éœ€çš„ç‰¹å¾åˆ—å
    full_feature_names = scaler.feature_names_in_

    # 3. ä¸¢å¼ƒä»»ä½•å«æœ‰ NaN çš„è¡Œ (ä»…æ£€æŸ¥æ¶‰åŠåˆ°çš„ç‰¹å¾åˆ—)
    # æˆ‘ä»¬é€‰æ‹©ç›´æŽ¥ä¸¢å¼ƒï¼Œå› ä¸ºå«æœ‰ inf çš„æµé‡é€šå¸¸æ˜¯æ— æ•ˆçš„ç»Ÿè®¡å¼‚å¸¸ï¼Œä¸é€‚åˆç”¨äºŽè®­ç»ƒç”Ÿæˆå™¨
    df_train_full.dropna(subset=full_feature_names, inplace=True)

    new_len = len(df_train_full)
    if new_len < original_len:
        print(f"âš ï¸ è­¦å‘Š: ä¸¢å¼ƒäº† {original_len - new_len} æ¡åŒ…å«æ— ç©·å¤§/ç¼ºå¤±å€¼çš„è„æ•°æ®ã€‚")
        print(f"   (å‰©ä½™æœ‰æ•ˆæ ·æœ¬: {new_len})")
    # ==========================================================

    # --- 2. å‡†å¤‡ç‰¹å¾(X)å’Œæ¡ä»¶æ ‡ç­¾(C) ---
    # ä½¿ç”¨ DataFrame è¿›è¡Œ transformï¼Œå¯ä»¥ä¿ç•™ç‰¹å¾åï¼Œæ¶ˆé™¤ Warning
    X_full_scaled = scaler.transform(df_train_full[full_feature_names])
    df_full_scaled = pd.DataFrame(X_full_scaled, columns=full_feature_names)

    X_scaled = df_full_scaled[ATTACKER_KNOWLEDGE_SET].values
    y_labels = df_train_full['label'].values

    print("æ•°æ®å‡†å¤‡å®Œæ¯•ã€‚")

    C_one_hot = np.zeros((len(y_labels), NUM_CLASSES))
    C_one_hot[np.arange(len(y_labels)), y_labels] = 1

    # --- 3. åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›† ---
    X_train, X_val, C_train, C_val = train_test_split(
        X_scaled, C_one_hot, test_size=VALIDATION_SPLIT, random_state=2025,
        stratify=C_one_hot.argmax(axis=1)
    )

    # --- 4. è®¡ç®—é‡‡æ ·æƒé‡ (å‡è¡¡é‡‡æ ·) ---
    print("[å…³é”®] æ­£åœ¨è®¡ç®— WeightedRandomSampler æƒé‡...")
    train_targets = C_train.argmax(axis=1)
    class_counts = np.bincount(train_targets)

    # é˜²æ­¢æŸä¸ªç±»åˆ«è¢«æ¸…æ´—æ²¡äº†å¯¼è‡´é™¤0é”™è¯¯
    if len(class_counts) < 2 or 0 in class_counts:
        print("é”™è¯¯: æ•°æ®æ¸…æ´—åŽæŸä¸ªç±»åˆ«çš„æ ·æœ¬æ•°ä¸º0ï¼Œè¯·æ£€æŸ¥æ•°æ®æºã€‚")
        return

    print(f"   -> è®­ç»ƒé›†åˆ†å¸ƒ: Benign={class_counts[0]}, Bot={class_counts[1]}")

    class_weights = 1. / class_counts
    class_weights = torch.tensor(class_weights, dtype=torch.float)
    train_sample_weights = class_weights[train_targets]

    sampler = WeightedRandomSampler(
        weights=train_sample_weights,
        num_samples=len(train_sample_weights),
        replacement=True
    )

    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),
                                  torch.tensor(C_train, dtype=torch.float32))
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, shuffle=False)

    val_tensor_x = torch.tensor(X_val, dtype=torch.float32).to(device)
    val_tensor_c = torch.tensor(C_val, dtype=torch.float32).to(device)

    # --- 5. è®­ç»ƒæ¨¡åž‹ ---
    model = ConditionalAutoencoder(FEATURE_DIM, LATENT_DIM, NUM_CLASSES).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()

    print("\nå¼€å§‹è®­ç»ƒCAEæ¨¡åž‹...")
    best_val_loss = float('inf')

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        for x_batch, c_batch in train_loader:
            x_batch, c_batch = x_batch.to(device), c_batch.to(device)
            recon, _ = model(x_batch, c_batch)
            loss = criterion(recon, x_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        model.eval()
        with torch.no_grad():
            recon_val, _ = model(val_tensor_x, val_tensor_c)
            val_loss = criterion(recon_val, val_tensor_x).item()
            if (epoch + 1) % 10 == 0:
                print(
                    f"  -> Epoch {epoch + 1:3d}/{EPOCHS}, Train Loss: {total_loss / len(train_loader):.6f}, Val Loss: {val_loss:.6f}")

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), CAE_MODEL_PATH)

    print("\n--- è®­ç»ƒå®Œæˆ ---")
    print(f"CAEæ¨¡åž‹å·²ä¿å­˜: {CAE_MODEL_PATH}")


if __name__ == "__main__":
    main()