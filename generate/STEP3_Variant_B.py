# generate/STEP3_Variant_B_no_constraint.py
import pandas as pd
import numpy as np
import os
import sys
import joblib
import torch
from sklearn.cluster import KMeans  # ä¿ç•™èšç±»

project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path: sys.path.append(project_root)

from models.style_transfer_cae import ConditionalAutoencoder
from models.lstm_finetuner import LSTMFinetuner
from models.lstm_predictor import LSTMPredictor
from config import DEFENDER_SET, ATTACKER_KNOWLEDGE_SET, ATTACKER_ACTION_SET, COMPLEX_SET, set_seed

# --- é…ç½® ---
# (è·¯å¾„ä¿æŒä¸å˜ï¼Œé™¤äº† Output)
CLEAN_DATA_PATH = os.path.join(project_root, 'data', 'splits', 'training_set.csv')
SCALER_PATH = os.path.join(project_root, 'models', 'global_scaler.pkl')
CAE_MODEL_PATH = os.path.join(project_root, 'models', 'style_transfer_cae.pt')
LSTM_FINETUNER_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_finetuner.pt')
PREDICTOR_MODEL_PATH = os.path.join(project_root, 'models', 'lstm_reconciliation_predictor.pt')

# è¾“å‡ºæ–‡ä»¶æ”¹ä¸º Variant B
OUTPUT_CSV_PATH = os.path.join(project_root, 'data', 'generated', 'variant_B_no_constraint.csv')

FEATURE_DIM_CAE = len(ATTACKER_KNOWLEDGE_SET)
LATENT_DIM_CAE = 5
NUM_CLASSES_CAE = 2
INPUT_DIM_LSTM_FINETUNER = len(ATTACKER_KNOWLEDGE_SET)
OUTPUT_DIM_LSTM_FINETUNER = len(ATTACKER_ACTION_SET)
INPUT_DIM_PREDICTOR = len(ATTACKER_ACTION_SET)
OUTPUT_DIM_PREDICTOR = len(COMPLEX_SET)

NUM_TO_GENERATE = 40000
MIMIC_INTENSITY = 0.98
NUM_BOT_CLUSTERS = 5
WATERMARK_KEY = 97
WATERMARK_FEATURE = 'Flow Duration'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def inject_watermark(df, key, feature_name):
    # æ°´å°é€»è¾‘ä¸­åŒ…å«äº†ä¸€äº›åŸºæœ¬çš„ Bytes/s è®¡ç®—ï¼Œè¿™é‡Œæˆ‘ä»¬ä¿ç•™å®ƒ
    # å› ä¸ºå¦‚æœä¸ç®—ï¼Œæ°´å°å¯èƒ½ä¼šå¯¼è‡´ Duration å˜äº†ä½† Rate æ²¡å˜ï¼Œè¿™æœ¬èº«å°±æ˜¯ä¸€ç§å¾®å°çš„ç¡¬çº¦æŸ
    # ä½†æ ¸å¿ƒçš„ STEP 6 è¢«ç§»é™¤äº†ï¼Œæ‰€ä»¥å¯¹æ¯”ä¾ç„¶æœ‰æ•ˆ
    print(f"\nğŸŒŠ [æ­¥éª¤7] æ³¨å…¥æ°´å° (Variant B)...")
    df_w = df.copy()
    values = df_w[feature_name].values.astype(int)
    residuals = values % key
    new_values = values - residuals
    mask_too_small = (new_values <= 0)
    new_values[mask_too_small] += key
    df_w[feature_name] = new_values

    # åœ¨ Variant B ä¸­ï¼Œæˆ‘ä»¬åªæ›´æ–° Durationï¼Œ**ä¸æ›´æ–°** Bytes/s å’Œ Pkts/s
    # è¿™æ ·æ›´èƒ½ä½“ç°"æ— çº¦æŸ"å¸¦æ¥çš„ä¸è‡ªæ´½æ€§
    print("   -> (æ³¨æ„: Variant B ä¸ä¼šåŒæ­¥æ›´æ–°å…³è”ç‰¹å¾ï¼Œæ•…æ„ä¿ç•™ä¸è‡ªæ´½æ€§)")

    return df_w


def main():
    set_seed(2025)
    print("=" * 60)
    print("ğŸš€ [æ¶ˆèå®éªŒ Variant B] æ— ç¡¬çº¦æŸ (No Hard Constraints)...")
    print("=" * 60)

    # 1. åŠ è½½ (ä¸å˜)
    scaler = joblib.load(SCALER_PATH)
    predictor = LSTMPredictor(INPUT_DIM_PREDICTOR, OUTPUT_DIM_PREDICTOR).to(device)
    predictor.load_state_dict(torch.load(PREDICTOR_MODEL_PATH, map_location=device))
    predictor.eval()
    cae_model = ConditionalAutoencoder(FEATURE_DIM_CAE, LATENT_DIM_CAE, NUM_CLASSES_CAE).to(device)
    cae_model.load_state_dict(torch.load(CAE_MODEL_PATH, map_location=device))
    cae_model.eval()
    lstm_finetuner = LSTMFinetuner(INPUT_DIM_LSTM_FINETUNER, OUTPUT_DIM_LSTM_FINETUNER).to(device)
    lstm_finetuner.load_state_dict(torch.load(LSTM_FINETUNER_MODEL_PATH, map_location=device))
    lstm_finetuner.eval()

    df_clean_full = pd.read_csv(CLEAN_DATA_PATH)
    df_benign_source = df_clean_full[df_clean_full['label'] == 0].sample(n=NUM_TO_GENERATE, replace=True,
                                                                         random_state=2025)
    df_bot_all = df_clean_full[df_clean_full['label'] == 1]

    # 1.5 èšç±» (ä¿ç•™)
    bot_scaled_full = scaler.transform(df_bot_all[DEFENDER_SET])
    kmeans = KMeans(n_clusters=NUM_BOT_CLUSTERS, random_state=2025, n_init=10)
    kmeans.fit(bot_scaled_full)
    centers_unscaled = scaler.inverse_transform(kmeans.cluster_centers_)
    df_bot_centers = pd.DataFrame(centers_unscaled, columns=DEFENDER_SET)
    tutor_indices = np.random.randint(0, NUM_BOT_CLUSTERS, size=NUM_TO_GENERATE)
    df_bot_tutors = df_bot_centers.iloc[tutor_indices].reset_index(drop=True)

    # 2. é£æ ¼æ¤å…¥ (ä¸å˜)
    with torch.no_grad():
        source_scaled = scaler.transform(df_benign_source[DEFENDER_SET])
        df_source_scaled = pd.DataFrame(source_scaled, columns=DEFENDER_SET)
        X_benign = torch.tensor(df_source_scaled[ATTACKER_KNOWLEDGE_SET].values, dtype=torch.float32).to(device)
        c_benign = torch.tensor([1.0, 0.0], dtype=torch.float32).expand(len(X_benign), -1).to(device)
        z_benign = cae_model.encode(X_benign, c_benign)

        tutors_scaled = scaler.transform(df_bot_tutors[DEFENDER_SET])
        df_tutors_scaled = pd.DataFrame(tutors_scaled, columns=DEFENDER_SET)
        X_bot = torch.tensor(df_tutors_scaled[ATTACKER_KNOWLEDGE_SET].values, dtype=torch.float32).to(device)
        c_bot_input = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(X_bot), -1).to(device)
        z_bot = cae_model.encode(X_bot, c_bot_input)

        z_hybrid = (1 - MIMIC_INTENSITY) * z_benign + MIMIC_INTENSITY * z_bot
        c_bot_target = torch.tensor([0.0, 1.0], dtype=torch.float32).expand(len(z_hybrid), -1).to(device)
        generated_knowledge_features_scaled = cae_model.decode(z_hybrid, c_bot_target)

    # 3. LSTM (ä¸å˜)
    with torch.no_grad():
        input_for_lstm = generated_knowledge_features_scaled.unsqueeze(1)
        refined_action = lstm_finetuner(input_for_lstm)
        df_knowledge_scaled = pd.DataFrame(generated_knowledge_features_scaled.cpu().numpy(),
                                           columns=ATTACKER_KNOWLEDGE_SET)
        original_action = torch.tensor(df_knowledge_scaled[ATTACKER_ACTION_SET].values, dtype=torch.float32).to(device)
        fused_action = 0.3 * original_action + 0.7 * refined_action
        fused_action = np.clip(fused_action.cpu().numpy(), 0, 1)

    # 4. é¢„æµ‹ (ä¸å˜)
    with torch.no_grad():
        input_predictor = torch.tensor(fused_action, dtype=torch.float32).unsqueeze(1).to(device)
        predicted_complex = predictor(input_predictor).cpu().numpy()
        predicted_complex = np.clip(predicted_complex, 0, 1)

    # 5. é€†å‘ç¼©æ”¾ (ä¸å˜)
    df_temp_action = pd.DataFrame(0, index=range(NUM_TO_GENERATE), columns=DEFENDER_SET)
    df_temp_action[ATTACKER_ACTION_SET] = fused_action
    action_unscaled = pd.DataFrame(scaler.inverse_transform(df_temp_action), columns=DEFENDER_SET)[ATTACKER_ACTION_SET]

    df_temp_complex = pd.DataFrame(0, index=range(NUM_TO_GENERATE), columns=DEFENDER_SET)
    df_temp_complex[COMPLEX_SET] = predicted_complex
    complex_unscaled = pd.DataFrame(scaler.inverse_transform(df_temp_complex), columns=DEFENDER_SET)[COMPLEX_SET]

    df_final = pd.concat([action_unscaled, complex_unscaled], axis=1)

    # --- âŒ ç§»é™¤ç¡¬çº¦æŸ ---
    print("\n[æ­¥éª¤6] è·³è¿‡ç¡¬çº¦æŸæ ¡å‡† (Ablation: No Constraints)...")
    # ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œé¢„æµ‹çš„åŸå§‹å€¼ï¼Œä¸åšæ•°å­¦ä¿®æ­£
    # ä»…è¡¥å…¨ç¼ºå¤±åˆ— (ä¸»è¦æ˜¯é‚£äº› Calculated Set é‡Œçš„ç‰¹å¾ï¼Œå¦‚ Bytes/sï¼Œä¼šè¢«è¡¥0æˆ–ä¿æŒNaN)
    for col in DEFENDER_SET:
        if col not in df_final.columns:
            # ç®€å•è¡¥0ï¼Œæˆ–è€…å¦‚æœä¸è¡¥0 Scalerä¼šæŠ¥é”™å—ï¼Ÿ
            # è¿˜æ˜¯å°½é‡ç®—ä¸€ä¸‹åŸºç¡€çš„å§ï¼Œä¸ç„¶æ¨¡å‹å¯èƒ½ç›´æ¥æŠ¥é”™
            # ä¸ºäº†ä½“ç°"æ²¡æœ‰å¼ºåˆ¶çº¦æŸ"ï¼Œæˆ‘ä»¬åªåšæœ€åŸºæœ¬çš„è¡¥å…¨ï¼Œä¸åšä¿®æ­£
            df_final[col] = 0

    df_final = df_final[DEFENDER_SET]

    # 7. æ°´å° (ä¿®æ”¹ç‰ˆï¼Œä¸æ›´æ–°å…³è”ç‰¹å¾)
    df_final_watermarked = inject_watermark(df_final, WATERMARK_KEY, WATERMARK_FEATURE)

    df_final_watermarked.to_csv(OUTPUT_CSV_PATH, index=False)
    print(f"\nâœ… Variant B ç”Ÿæˆå®Œæ¯•: {OUTPUT_CSV_PATH}")


if __name__ == "__main__":
    main()